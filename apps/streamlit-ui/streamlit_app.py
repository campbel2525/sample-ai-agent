import os
import json
import time
from typing import Any, Dict, List, Optional

import requests
import streamlit as st
from streamlit.components.v1 import html as st_html

# =============================
# å®šæ•°ï¼ˆä¸Šéƒ¨ã«é›†ç´„ï¼‰
# =============================
# FastAPIã®å›ºå®šURLï¼ˆç’°å¢ƒå¤‰æ•°ã¯ç„¡è¦–ã—ã¦å›ºå®šï¼‰
DEFAULT_FASTAPI_BASE_URL = "http://ai-agent:8000"

# RAGasã®æ—¢å®šã¯OFF
DEFAULT_RUN_RAGAS = False

# ãƒ¢ãƒ‡ãƒ«åã®æ—¢å®šå€¤ï¼ˆæœªå…¥åŠ›æ™‚ã¯APIå´ã®æ—¢å®šã§ã‚‚å‹•ããŒã€UIã§ã¯æ˜ç¤ºï¼‰
DEFAULT_PLANNER_MODEL = "gpt-4o-2024-08-06"
DEFAULT_SUBTASK_TOOL_SELECTION_MODEL = "gpt-4o-2024-08-06"
DEFAULT_SUBTASK_ANSWER_MODEL = "gpt-4o-2024-08-06"
DEFAULT_SUBTASK_REFLECTION_MODEL = "gpt-4o-2024-08-06"
DEFAULT_FINAL_ANSWER_MODEL = "gpt-4o-2024-08-06"

# UIã®é«˜ã•ãƒ»ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
PROMPT_TEXTAREA_HEIGHT = 120
PARAMS_TEXTAREA_HEIGHT = 80
REQUEST_TIMEOUT_SEC = 600

# APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
EXEC_ENDPOINT = "/ai_agents/chatbot/exec"


def init_state():
    if "messages" not in st.session_state:
        st.session_state.messages: List[Dict[str, str]] = []  # {role, content}
    if "last_request" not in st.session_state:
        st.session_state.last_request: Optional[Dict[str, Any]] = None
    if "last_response" not in st.session_state:
        st.session_state.last_response: Optional[Dict[str, Any]] = None
    if "turns" not in st.session_state:
        # å„ã‚¿ãƒ¼ãƒ³ã®è©³ç´°ï¼ˆuserç™ºè©±ãƒ»assistantå¿œç­”ãƒ»è©³ç´°çµæœï¼‰ã‚’ä¿æŒ
        st.session_state.turns: List[Dict[str, Any]] = []


def to_chat_history(messages: List[Dict[str, str]]) -> List[Dict[str, str]]:
    # FastAPIã®ChatCompletionMessageParamã«åˆã‚ã›ãŸæœ€ä½é™ã®å½¢å¼
    out = []
    for m in messages:
        if m.get("role") in ("user", "assistant") and m.get("content"):
            out.append({"role": m["role"], "content": m["content"]})
    return out


def parse_json_or_none(label: str, raw: str) -> Optional[Dict[str, Any]]:
    if not raw:
        return None
    try:
        data = json.loads(raw)
        if data is None:
            return None
        if not isinstance(data, dict):
            st.warning(f"{label} ã¯ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ(JSON)ã§æŒ‡å®šã—ã¦ãã ã•ã„ã€‚ç„¡è¦–ã—ã¾ã™ã€‚")
            return None
        return data
    except Exception as e:
        st.error(f"{label} ã®JSONãŒä¸æ­£ã§ã™: {e}")
        return None


def main():
    st.set_page_config(page_title="AI Agent Chat UI", page_icon="ğŸ¤–", layout="wide")
    init_state()
    # ãƒšãƒ³ãƒ‡ã‚£ãƒ³ã‚°é€ä¿¡ã®å‡¦ç†ï¼ˆå…ˆã«å®Ÿè¡Œã—ã¦ã‹ã‚‰æç”»ï¼‰
    pending_payload = st.session_state.pop("pending_payload", None)
    if pending_payload is not None:
        # APIå‘¼ã³å‡ºã—ï¼ˆä¼šè©±ã¯ä¸Šå´ã«æç”»ã•ã‚Œã€ãã®ä¸‹ã«å…¥åŠ›æ¬„ãŒæ¥ã‚‹ï¼‰
        try:
            t0 = time.time()
            resp = requests.post(
                f"{DEFAULT_FASTAPI_BASE_URL}{EXEC_ENDPOINT}",
                json=pending_payload,
                timeout=REQUEST_TIMEOUT_SEC,
            )
            latency = time.time() - t0
            if resp.status_code == 200:
                data = resp.json()
                st.session_state.last_request = pending_payload
                st.session_state.last_response = data
                answer = data.get("answer") or ""
                # ä¼šè©±ã«assistantã‚’è¿½åŠ 
                st.session_state.messages.append(
                    {"role": "assistant", "content": answer}
                )
                # ã‚¿ãƒ¼ãƒ³å±¥æ­´
                st.session_state.turns.append(
                    {
                        "user": pending_payload.get("query", ""),
                        "assistant": answer,
                        "detail": {
                            "latency_sec": round(latency, 2),
                            "raw_response": data,
                            "plan": (data.get("ai_agent_result") or {}).get("plan"),
                            "subtasks": (data.get("ai_agent_result") or {}).get(
                                "subtasks_detail"
                            ),
                            "ragas_scores": (data.get("ragas_result") or {}).get(
                                "scores"
                            )
                            or {},
                            "langfuse_session_id": data.get("langfuse_session_id"),
                        },
                    }
                )
            else:
                st.error(f"APIã‚¨ãƒ©ãƒ¼: {resp.status_code} {resp.text}")
                st.session_state.messages.append(
                    {
                        "role": "assistant",
                        "content": "ã™ã¿ã¾ã›ã‚“ã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                    }
                )
        except Exception as e:
            st.error(f"é€šä¿¡ã‚¨ãƒ©ãƒ¼: {e}")
            st.session_state.messages.append(
                {
                    "role": "assistant",
                    "content": "ã™ã¿ã¾ã›ã‚“ã€é€šä¿¡ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                }
            )

    # å…¥åŠ›æ¬„ã®å›ºå®šã¯æç”»å´©ã‚Œã®ãŸã‚ä¸€æ—¦ã‚ªãƒ•ï¼ˆæœ€ä¸‹éƒ¨ã«é€šå¸¸è¡¨ç¤ºï¼‰

    # å›ºå®šURLï¼ˆå…¥åŠ›æ¬„ã¯å»ƒæ­¢ï¼‰
    with st.sidebar:
        # å‚è€ƒãƒªãƒ³ã‚¯ï¼ˆFastAPIã®APIä»•æ§˜ï¼‰
        st.caption("è©³ã—ãã¯ http://localhost:8000/docs å‚ç…§")

        st.subheader("RAGas")
        is_run_ragas = st.checkbox("RAGasã‚’å®Ÿè¡Œã™ã‚‹", value=DEFAULT_RUN_RAGAS)
        ragas_reference = st.text_area("RAGas reference", value="")

        st.subheader("ãƒ¢ãƒ‡ãƒ«è¨­å®š (æœªå…¥åŠ›ã¯APIæ—¢å®š)")
        planner_model_name = st.text_input(
            "planner_model_name", value=DEFAULT_PLANNER_MODEL
        )
        subtask_tool_selection_model_name = st.text_input(
            "subtask_tool_selection_model_name",
            value=DEFAULT_SUBTASK_TOOL_SELECTION_MODEL,
        )
        subtask_answer_model_name = st.text_input(
            "subtask_answer_model_name", value=DEFAULT_SUBTASK_ANSWER_MODEL
        )
        subtask_reflection_model_name = st.text_input(
            "subtask_reflection_model_name", value=DEFAULT_SUBTASK_REFLECTION_MODEL
        )
        final_answer_model_name = st.text_input(
            "final_answer_model_name", value=DEFAULT_FINAL_ANSWER_MODEL
        )

        st.subheader("ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿(JSON) (æœªå…¥åŠ›ã¯None)")
        planner_params_raw = st.text_area(
            "planner_params", height=PARAMS_TEXTAREA_HEIGHT
        )
        subtask_tool_selection_params_raw = st.text_area(
            "subtask_tool_selection_params", height=PARAMS_TEXTAREA_HEIGHT
        )
        subtask_answer_params_raw = st.text_area(
            "subtask_answer_params", height=PARAMS_TEXTAREA_HEIGHT
        )
        subtask_reflection_params_raw = st.text_area(
            "subtask_reflection_params", height=PARAMS_TEXTAREA_HEIGHT
        )
        final_answer_params_raw = st.text_area(
            "final_answer_params", height=PARAMS_TEXTAREA_HEIGHT
        )

        st.subheader("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¸Šæ›¸ã (æœªå…¥åŠ›ã¯æ—¢å®š)")
        with st.expander("Planner prompts"):
            ai_agent_planner_system_prompt = st.text_area(
                "ai_agent_planner_system_prompt", height=PROMPT_TEXTAREA_HEIGHT
            )
            ai_agent_planner_user_prompt = st.text_area(
                "ai_agent_planner_user_prompt", height=PROMPT_TEXTAREA_HEIGHT
            )
        with st.expander("Subtask prompts"):
            ai_agent_subtask_system_prompt = st.text_area(
                "ai_agent_subtask_system_prompt", height=PROMPT_TEXTAREA_HEIGHT
            )
            ai_agent_subtask_tool_selection_user_prompt = st.text_area(
                "ai_agent_subtask_tool_selection_user_prompt",
                height=PROMPT_TEXTAREA_HEIGHT,
            )
            ai_agent_subtask_reflection_user_prompt = st.text_area(
                "ai_agent_subtask_reflection_user_prompt", height=PROMPT_TEXTAREA_HEIGHT
            )
            ai_agent_subtask_retry_answer_user_prompt = st.text_area(
                "ai_agent_subtask_retry_answer_user_prompt",
                height=PROMPT_TEXTAREA_HEIGHT,
            )
        with st.expander("Final answer prompts"):
            ai_agent_create_last_answer_system_prompt = st.text_area(
                "ai_agent_create_last_answer_system_prompt",
                height=PROMPT_TEXTAREA_HEIGHT,
            )
            ai_agent_create_last_answer_user_prompt = st.text_area(
                "ai_agent_create_last_answer_user_prompt", height=PROMPT_TEXTAREA_HEIGHT
            )

    st.title("ğŸ¤– Chatbot AI Agent")

    # æ—¢å­˜ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºï¼ˆè©³ç´°ã”ã¨ä¿æŒã—ã¦ã„ã‚‹å ´åˆã¯ãã¡ã‚‰ã‚’å„ªå…ˆï¼‰
    if st.session_state.turns:
        for t in st.session_state.turns:
            user_text = t.get("user", "")
            asst_text = t.get("assistant", "")
            detail = t.get("detail", {})

            if user_text:
                with st.chat_message("user"):
                    st.markdown(user_text)
            if asst_text:
                with st.chat_message("assistant"):
                    st.markdown(asst_text)
                    if detail:
                        # ãƒ•ãƒ«APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’è¡¨ç¤º
                        raw_resp = detail.get("raw_response")
                        if raw_resp is not None:
                            with st.expander("APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆrawï¼‰"):
                                st.json(raw_resp)
                        # å‚è€ƒï¼šãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ç­‰ã®è»½é‡ãƒ¡ã‚¿
                        if "latency_sec" in detail:
                            st.caption(f"latency: {detail['latency_sec']}s")
    else:
        for m in st.session_state.messages:
            with st.chat_message(m["role"]):
                st.markdown(m["content"])

    # å…¥åŠ›æ¬„ï¼ˆãƒãƒ£ãƒƒãƒˆã®ç›´ä¸‹ã«é…ç½®ãƒ»ç”»é¢æœ€ä¸‹éƒ¨ã«å›ºå®šï¼‰
    # Streamlitã¯HTMLã®ãƒã‚¹ãƒˆã‚’ç¶­æŒã—ãªã„ãŸã‚ã€stFormã‚’ç›´æ¥å›ºå®šåŒ–ã™ã‚‹CSSã‚’é©ç”¨
    st.markdown(
        """
        <style>
        :root { --footer-height: 140px; }
        /* æœ¬æ–‡ãŒãƒ•ãƒƒã‚¿ãƒ¼ã§éš ã‚Œãªã„ã‚ˆã†ã«ä¸‹ä½™ç™½ */
        section.main > div.block-container { padding-bottom: var(--footer-height); }
        /* ãƒšãƒ¼ã‚¸å†…ã®stFormã‚’å›ºå®šãƒ•ãƒƒã‚¿ãƒ¼åŒ–ï¼ˆã“ã®ã‚¢ãƒ—ãƒªã§ã¯1ã¤ã®ã¿ï¼‰ */
        section.main div[data-testid="stForm"] {
          position: fixed; left: 0; right: 0; bottom: 0; z-index: 1000;
          padding: 10px 16px; background: var(--footer-bg, rgba(255,255,255,0.97));
          box-shadow: 0 -2px 10px rgba(0,0,0,0.12);
        }
        /* ä¸­èº«ã‚’ä¸­å¤®å¯„ã›ï¼ˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„å¹…ã¨æƒãˆã‚‹ãŸã‚ã®æ§ãˆã‚ãªæœ€å¤§å¹…ï¼‰ */
        section.main div[data-testid="stForm"] > div { max-width: 1000px; margin: 0 auto; }
        </style>
        """,
        unsafe_allow_html=True,
    )
    with st.form("chat_form", clear_on_submit=True):
        chat_value = st.text_area(
            "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸",
            key="chat_input_area",
            height=100,
            placeholder="ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å…¥åŠ›â€¦ (é€ä¿¡: âŒ˜/Ctrl + Enter)",
            label_visibility="collapsed",
        )
        submitted = st.form_submit_button("é€ä¿¡", type="primary")

    # Cmd/Ctrl+Enter ã§é€ä¿¡ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã™ã‚‹JSï¼ˆç°¡æ˜“ï¼‰
    st_html(
        """
        <script>
        (function(){
          // èƒŒæ™¯è‰²ã‚’ãƒ†ãƒ¼ãƒã«åˆã‚ã›ã‚‹
          try{
            const bg = getComputedStyle(parent.document.body).backgroundColor;
            const forms = parent.document.querySelectorAll('section.main div[data-testid="stForm"]');
            forms.forEach(f => f.style.background = bg);
          }catch(_){ }

          function clickSend(){
            const btns = parent.document.querySelectorAll('button');
            for(let i=btns.length-1;i>=0;i--){
              const t = (btns[i].innerText||'').trim();
              if(t === 'é€ä¿¡'){ btns[i].click(); break; }
            }
          }
          window.addEventListener('keydown', function(e){
            if ((e.metaKey||e.ctrlKey) && e.key === 'Enter') { e.preventDefault(); clickSend(); }
          }, true);
        })();
        </script>
        """,
        height=0,
    )

    # é€ä¿¡å‡¦ç†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ ã¯ãƒšãƒ¼ã‚¸æœ€ä¸‹éƒ¨ã«1ã¤ã ã‘ï¼‰ã€‚å³æ™‚APIã¯å©ã‹ãšpayloadã‚’ä¿å­˜â†’å†æç”»ã®å…ˆé ­ã§å‡¦ç†
    if submitted and chat_value and chat_value.strip():
        user_input: str = chat_value.strip()

        # JSONãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è§£æ
        planner_params = parse_json_or_none("planner_params", planner_params_raw)
        subtask_tool_selection_params = parse_json_or_none(
            "subtask_tool_selection_params", subtask_tool_selection_params_raw
        )
        subtask_answer_params = parse_json_or_none(
            "subtask_answer_params", subtask_answer_params_raw
        )
        subtask_reflection_params = parse_json_or_none(
            "subtask_reflection_params", subtask_reflection_params_raw
        )
        final_answer_params = parse_json_or_none(
            "final_answer_params", final_answer_params_raw
        )

        def nvl(s: str) -> Optional[str]:
            return s if s else None

        # RAGas å®Ÿè¡Œå¯å¦ã¨å‚ç…§ã®æ•´å½¢
        ragas_ref_trim = (ragas_reference or "").strip()
        # ãƒã‚§ãƒƒã‚¯ãŒå…¥ã£ã¦ã„ã‚‹ã®ã«å‚ç…§ãŒç©ºãªã‚‰ã€APIãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã§422ã«ãªã‚‹ãŸã‚äº‹å‰ã«è­¦å‘Šã—ã¦é€ä¿¡ã—ãªã„
        if is_run_ragas and not ragas_ref_trim:
            st.warning("RAGasã‚’å®Ÿè¡Œã™ã‚‹ã«ã¯ 'RAGas reference' ã®å…¥åŠ›ãŒå¿…è¦ã§ã™ã€‚")
            return

        # é€ä¿¡å¯ã¨ãªã£ãŸæ®µéšã§ãƒ¦ãƒ¼ã‚¶ãƒ¼ç™ºè©±ã‚’å±¥æ­´ã«è¿½åŠ ã—ã€å±¥æ­´ã‚’ä½œæˆ
        st.session_state.messages.append({"role": "user", "content": user_input})
        chat_history = to_chat_history(st.session_state.messages[:-1])

        payload: Dict[str, Any] = {
            "query": user_input,
            "chat_history": chat_history,
            "planner_model_name": nvl(planner_model_name),
            "subtask_tool_selection_model_name": nvl(subtask_tool_selection_model_name),
            "subtask_answer_model_name": nvl(subtask_answer_model_name),
            "subtask_reflection_model_name": nvl(subtask_reflection_model_name),
            "final_answer_model_name": nvl(final_answer_model_name),
            "planner_params": planner_params,
            "subtask_tool_selection_params": subtask_tool_selection_params,
            "subtask_answer_params": subtask_answer_params,
            "subtask_reflection_params": subtask_reflection_params,
            "final_answer_params": final_answer_params,
            "ai_agent_planner_system_prompt": nvl(ai_agent_planner_system_prompt),
            "ai_agent_planner_user_prompt": nvl(ai_agent_planner_user_prompt),
            "ai_agent_subtask_system_prompt": nvl(ai_agent_subtask_system_prompt),
            "ai_agent_subtask_tool_selection_user_prompt": nvl(
                ai_agent_subtask_tool_selection_user_prompt
            ),
            "ai_agent_subtask_reflection_user_prompt": nvl(
                ai_agent_subtask_reflection_user_prompt
            ),
            "ai_agent_subtask_retry_answer_user_prompt": nvl(
                ai_agent_subtask_retry_answer_user_prompt
            ),
            "ai_agent_create_last_answer_system_prompt": nvl(
                ai_agent_create_last_answer_system_prompt
            ),
            "ai_agent_create_last_answer_user_prompt": nvl(
                ai_agent_create_last_answer_user_prompt
            ),
            # ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã®å€¤ã‚’ãã®ã¾ã¾æ¸¡ã™ï¼ˆäº‹å‰ã«å‚ç…§ã®å¿…é ˆãƒã‚§ãƒƒã‚¯æ¸ˆã¿ï¼‰
            "is_run_ragas": is_run_ragas,
            "ragas_reference": ragas_ref_trim if ragas_ref_trim else None,
        }

        st.session_state["pending_payload"] = payload
        st.rerun()

    # ï¼ˆæ“ä½œãƒœã‚¿ãƒ³çœç•¥ï¼‰


if __name__ == "__main__":
    main()
