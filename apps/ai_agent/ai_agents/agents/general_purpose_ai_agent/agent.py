import json
import operator
from typing import (
    Annotated,
    Any,
    Iterable,
    Literal,
    Optional,
    Sequence,
    Type,
    TypedDict,
    cast,
)

from langchain_core.tools import BaseTool
from langchain_core.utils.function_calling import convert_to_openai_tool
from langgraph.constants import Send
from langgraph.graph import END, START, StateGraph
from langgraph.pregel import Pregel
from openai import OpenAI
from openai.types.chat import (
    ChatCompletionAssistantMessageParam,
    ChatCompletionMessageParam,
)
from pydantic import BaseModel

from .custom_logger import setup_logger
from .models import (
    AgentResult,
    AgentSetting,
    Plan,
    ReflectionResult,
    Subtask,
    ToolResult,
)

logger = setup_logger(__file__)


class AgentSubGraphState(TypedDict):
    """ã‚µãƒ–ã‚°ãƒ©ãƒ•ï¼ˆå˜ä¸€ã‚µãƒ–ã‚¿ã‚¹ã‚¯å®Ÿè¡Œï¼‰ã§ç”¨ã„ã‚‹çŠ¶æ…‹ã€‚

    å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«ã¤ã„ã¦ã€ãƒ„ãƒ¼ãƒ«é¸æŠâ†’å®Ÿè¡Œâ†’å›ç­”ç”Ÿæˆâ†’å†…çœã®
    ä¸€é€£ã®å‡¦ç†ã§å—ã‘æ¸¡ã™ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã—ã¾ã™ã€‚
    """

    query: str
    plan: list[str]
    subtask: str
    is_completed: bool
    messages: list[ChatCompletionMessageParam]
    challenge_count: int
    tool_results: Annotated[Sequence[Sequence[ToolResult]], operator.add]
    reflection_results: Annotated[Sequence[ReflectionResult], operator.add]
    subtask_answer: str


class AgentState(TypedDict):
    """ãƒ¡ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ï¼ˆå…¨ä½“å®Ÿè¡Œï¼‰ã§ç”¨ã„ã‚‹çŠ¶æ…‹ã€‚

    è¨ˆç”»ä½œæˆã€å„ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®é›†ç´„ã€æœ€çµ‚å›ç­”ä½œæˆã®ãŸã‚ã®
    å…¥åŠ›ãƒ»ä¸­é–“çµæœãƒ»æœ€çµ‚çµæœã‚’ä¿æŒã—ã¾ã™ã€‚
    """

    query: str
    chat_history: list[ChatCompletionMessageParam]
    plan: list[str]
    current_step: int
    subtask_results: Annotated[Sequence[Subtask], operator.add]
    answer: str


class Agent:
    """æ±ç”¨RAGã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚

    - è¨ˆç”»ä½œæˆï¼ˆè³ªå•åˆ†è§£ï¼‰
    - ã‚µãƒ–ã‚¿ã‚¹ã‚¯å®Ÿè¡Œï¼ˆãƒ„ãƒ¼ãƒ«é¸æŠ/å®Ÿè¡Œâ†’å›ç­”â†’å†…çœã®ç¹°ã‚Šè¿”ã—ï¼‰
    - æœ€çµ‚å›ç­”ä½œæˆï¼ˆå…¨ã‚µãƒ–ã‚¿ã‚¹ã‚¯çµæœã®çµ±åˆï¼‰

    ã‚’LangGraphã§æ§‹æˆã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚
    """

    def __init__(
        self,
        openai_base_url: str,
        openai_api_key: str,
        settings: AgentSetting | None = None,
        tools: list[BaseTool] = [],
        max_challenge_count: int = 3,
        # ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®æœ€å¤§ä½¿ç”¨ä»¶æ•°ï¼ˆNoneã§å…¨ä»¶ï¼‰
        chat_history_max_turns: Optional[int] = None,
    ) -> None:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹ã€‚

        Args:
            openai_base_url (str): OpenAIäº’æ›ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã®ãƒ™ãƒ¼ã‚¹URLã€‚
            openai_api_key (str): OpenAI APIã‚­ãƒ¼ã€‚
            settings (AgentSetting | None): å„ãƒ•ã‚§ãƒ¼ã‚ºã®ãƒ¢ãƒ‡ãƒ«/ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¨­å®šã€‚æœªæŒ‡å®šæ™‚ã¯æ—¢å®šå€¤ã€‚
            tools (list[BaseTool]): åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ä¸€è¦§ï¼ˆLangChain Toolï¼‰ã€‚
            max_challenge_count (int): å†…çœã«åŸºã¥ããƒªãƒˆãƒ©ã‚¤ã®æœ€å¤§å›æ•°ã€‚
        """
        self.openai_base_url = openai_base_url
        self.openai_api_key = openai_api_key
        self.settings = settings or AgentSetting()
        self.tools = tools
        self.tool_map = {tool.name: tool for tool in tools}

        # OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åˆæœŸåŒ–
        self.client = OpenAI(
            base_url=self.openai_base_url,
            api_key=self.openai_api_key,
        )

        self.max_challenge_count = max_challenge_count
        self.chat_history_max_turns = chat_history_max_turns

    def run_agent(
        self, query: str, chat_history: list[ChatCompletionMessageParam] = []
    ) -> AgentResult:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å®Ÿè¡Œã™ã‚‹

        Args:
            query (str): å…¥åŠ›ã®è³ªå•
            chat_history (list[dict], optional): ãƒãƒ£ãƒƒãƒˆå±¥æ­´

        Returns:
            AgentResult: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®Ÿè¡Œçµæœ
        """

        app = self.create_graph()
        result = app.invoke(
            {
                "query": query,
                "chat_history": chat_history,
                "current_step": 0,
            }
        )

        agent_result = AgentResult(
            query=query,
            plan=Plan(subtasks=result["plan"]),
            subtasks=result["subtask_results"],
            answer=result["answer"],
        )

        return agent_result

    def create_graph(self) -> Pregel:
        """ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã™ã‚‹

        Returns:
            Pregel: ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ¡ã‚¤ãƒ³ã‚°ãƒ©ãƒ•
        """
        workflow = StateGraph(AgentState)

        # Add the plan node
        workflow.add_node("create_plan", self._create_plan)

        # Add the execution step
        workflow.add_node("execute_subtasks", self._execute_subgraph)

        workflow.add_node("create_answer", self._create_answer)

        workflow.add_edge(START, "create_plan")

        # From plan we go to agent
        workflow.add_conditional_edges(
            "create_plan",
            self._should_continue_exec_subtasks,
        )

        # From agent, we replan
        workflow.add_edge("execute_subtasks", "create_answer")

        workflow.set_finish_point("create_answer")

        app = workflow.compile()

        return app

    def _create_plan(self, state: AgentState) -> dict:
        """1. è¨ˆç”»ä½œæˆï½œè³ªå•åˆ†è§£ã¨ã‚µãƒ–ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆä½œæˆ

        Args:
            state (AgentState): å…¥åŠ›ã®çŠ¶æ…‹

        Returns:
            AgentState: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting plan generation process...")

        # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ä½œæˆ
        planner_prompt = self.settings.planner.prompt
        conversation_context = self._format_chat_history(state.get("chat_history", []))
        messages: list[ChatCompletionMessageParam] = [
            {
                "role": "system",
                "content": planner_prompt.system_prompt.replace(
                    "{conversation_context}", conversation_context
                ),
            },
            {
                "role": "user",
                "content": planner_prompt.user_prompt.replace(
                    "{query}", str(state["query"])
                ),
            },
        ]

        logger.debug(f"Final prompt messages: {messages}")

        # OpenAIã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡
        try:
            logger.info("Sending request to OpenAI...")
            response = self._chat_parse(
                model=self.settings.planner.model_name,
                messages=messages,
                response_format=Plan,
                **self.settings.planner.model_params,
            )
            logger.info("âœ… Successfully received response from OpenAI.")
        except Exception as e:
            logger.error(f"Error during OpenAI request: {e}")
            raise

        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰Structured outputã‚’åˆ©ç”¨ã—Planã‚¯ãƒ©ã‚¹ã‚’å–å¾—
        plan = response.choices[0].message.parsed

        logger.info("Plan generation complete!")

        # ç”Ÿæˆã—ãŸè¨ˆç”»ã‚’è¿”ã—ã€çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹
        return {"plan": plan.subtasks}

    def _select_tools(self, state: AgentSubGraphState) -> dict:
        """2.1 ãƒ„ãƒ¼ãƒ«é¸æŠï½œLLMãŒé©åˆ‡ãªãƒ„ãƒ¼ãƒ«ã‚’åˆ¤æ–­ãƒ»é¸æŠ

        Args:
            state (AgentSubGraphState): å…¥åŠ›ã®çŠ¶æ…‹

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting tool selection process...")

        # OpenAIå¯¾å¿œã®toolå®šç¾©ã«æ›¸ãæ›ãˆã‚‹
        logger.debug("Converting tools for OpenAI format...")
        openai_tools = [convert_to_openai_tool(tool) for tool in self.tools]

        messages: list[ChatCompletionMessageParam]

        # ãƒªãƒˆãƒ©ã‚¤ã•ã‚ŒãŸã‹ã©ã†ã‹ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹
        if state["challenge_count"] == 0:
            logger.debug("Creating user prompt for tool selection...")
            subtask_prompt = self.settings.subtask_select_tool.prompt
            messages = [
                {
                    "role": "system",
                    "content": subtask_prompt.system_prompt,
                },
                {
                    "role": "user",
                    "content": subtask_prompt.user_prompt.replace(
                        "{query}", str(state["query"])
                    )
                    .replace("{plan}", str(state["plan"]))
                    .replace("{subtask}", str(state["subtask"])),
                },
            ]
            try:
                logger.info("Sending request to OpenAI...")
                response = self._chat_create(
                    model=self.settings.subtask_select_tool.model_name,
                    messages=messages,
                    tools=openai_tools,
                    **self.settings.subtask_select_tool.model_params,
                )
                logger.info(response.choices[0].message.tool_calls)
                logger.info("âœ… Successfully received response from OpenAI.")
            except Exception as e:
                logger.error(f"Error during OpenAI request: {e}")
                raise

        else:
            logger.debug("Creating user prompt for tool retry...")

            # NOTE: ãƒˆãƒ¼ã‚¯ãƒ³æ•°ç¯€ç´„ã®ãŸã‚éå»ã®æ¤œç´¢çµæœã¯é™¤ã
            # roleãŒtoolã¾ãŸã¯tool_callsã‚’æŒã¤ã‚‚ã®ã¯é™¤ã
            messages = [
                message
                for message in state["messages"]
                if message["role"] != "tool" and "tool_calls" not in message
            ]

            retry_prompt = self.settings.subtask_retry_answer.prompt
            messages.append({"role": "user", "content": retry_prompt.user_prompt})

            try:
                logger.info("Sending request to OpenAI...")
                response = self._chat_create(
                    model=self.settings.subtask_retry_answer.model_name,
                    messages=messages,
                    tools=openai_tools,
                    **self.settings.subtask_retry_answer.model_params,
                )
                logger.info(response.choices[0].message.tool_calls)
                logger.info("âœ… Successfully received response from OpenAI.")
            except Exception as e:
                logger.error(f"Error during OpenAI request: {e}")
                raise

        tool_calls = response.choices[0].message.tool_calls
        ai_message: ChatCompletionAssistantMessageParam = {
            "role": "assistant",
        }

        if tool_calls:
            ai_message["tool_calls"] = [tc.model_dump() for tc in tool_calls]
        else:
            ai_message["content"] = response.choices[0].message.content or ""

        logger.info("Tool selection complete!")
        messages.append(ai_message)

        return {"messages": messages}

    def _execute_tools(self, state: AgentSubGraphState) -> dict:
        """2.2 ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œï½œé¸æŠã—ãŸãƒ„ãƒ¼ãƒ«ã‚’å®Ÿè¡Œã€‚

        select_tools ã®çµæœï¼ˆç›´å‰ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰ã«å«ã¾ã‚Œã‚‹ `tool_calls` ã‚’é †ã«å®Ÿè¡Œã—ã€
        å„ãƒ„ãƒ¼ãƒ«ã®æˆ»ã‚Šå€¤ã‚’ `ToolResult` ã¨ã—ã¦è“„ç©ã—ã¾ã™ã€‚ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ãŒç„¡ã„å ´åˆã¯
        å®Ÿè¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã€ç©ºã®çµæœã‚’è¿”ã—ã¾ã™ã€‚

        Args:
            state (AgentSubGraphState): ã‚µãƒ–ã‚¿ã‚¹ã‚¯å®Ÿè¡Œä¸­ã®çŠ¶æ…‹ï¼ˆmessages ã‚’å«ã‚€ï¼‰ã€‚

        Returns:
            dict: ä»¥ä¸‹ã‚’å«ã‚€æ›´æ–°æ¸ˆã¿çŠ¶æ…‹ã®å·®åˆ†ã€‚
                - `messages`: ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœï¼ˆtoolãƒ­ãƒ¼ãƒ«ï¼‰ã‚’è¿½åŠ ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ—
                - `tool_results`: å®Ÿè¡Œã—ãŸãƒ„ãƒ¼ãƒ«çµæœï¼ˆList[List[ToolResult]]] å½¢å¼ï¼‰
        """

        logger.info("ğŸš€ Starting tool execution process...")
        messages = state["messages"]

        tool_calls = cast(Optional[list[Any]], messages[-1].get("tool_calls"))

        # â˜…ãƒ„ãƒ¼ãƒ«ãŒç„¡ã„ï¼ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç©ºçµæœã§å¾Œæ®µã®å‹ã‚’æº€ãŸã™ï¼‰
        if tool_calls is None or len(tool_calls) == 0:
            logger.warning("No tool calls found. Skipping tool execution.")
            return {"messages": messages, "tool_results": [[]]}

        # ä»¥é™ã¯æ—¢å­˜ã®å®Ÿè¡Œãƒ«ãƒ¼ãƒ—
        tool_results = []
        for tool_call in tool_calls:
            tool_name = tool_call["function"]["name"]
            tool_args_str = tool_call["function"]["arguments"]
            tool_args = json.loads(tool_args_str)

            tool = self.tool_map[tool_name]
            tool_result = tool.invoke(tool_args)

            tool_results.append(
                ToolResult(
                    tool_name=tool_name,
                    args=tool_args,
                    results=tool_result,
                )
            )

            messages.append(
                {
                    "role": "tool",
                    "content": str(tool_result),
                    "tool_call_id": tool_call["id"],
                }
            )

        logger.info("Tool execution complete!")
        return {"messages": messages, "tool_results": [tool_results]}

    def _create_subtask_answer(self, state: AgentSubGraphState) -> dict:
        """2.3 å›ç­”ç”Ÿæˆï½œãƒ„ãƒ¼ãƒ«å®Ÿè¡Œçµæœã‹ã‚‰å›ç­”ã‚’ä½œæˆ

        Args:
            state (AgentSubGraphState): å…¥åŠ›ã®çŠ¶æ…‹

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting subtask answer creation process...")
        messages = state["messages"]

        try:
            logger.info("Sending request to OpenAI...")
            # å›ç­”ç”Ÿæˆã¯ subtask_answer ã®ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼ˆretry ã¨åŒä¸€è¨­å®šã‚’æµç”¨ï¼‰
            response = self._chat_create(
                model=self.settings.subtask_retry_answer.model_name,
                messages=messages,
                **self.settings.subtask_retry_answer.model_params,
            )
            logger.info("âœ… Successfully received response from OpenAI.")
        except Exception as e:
            logger.error(f"Error during OpenAI request: {e}")
            raise

        subtask_answer = response.choices[0].message.content

        ai_message = cast(
            ChatCompletionMessageParam,
            {
                "role": "assistant",
                "content": subtask_answer,
            },
        )
        messages.append(ai_message)

        logger.info("Subtask answer creation complete!")

        return {
            "messages": messages,
            "subtask_answer": subtask_answer,
        }

    def _reflect_subtask(self, state: AgentSubGraphState) -> dict:
        """2.4 è‡ªå·±ä¿®æ­£ï½œå›ç­”ã®é©åˆ‡æ€§è©•ä¾¡ã¨åŸå› åˆ†æâ†’å†è©¦è¡ŒæŒ‡ç¤º

        Args:
            state (AgentSubGraphState): å…¥åŠ›ã®çŠ¶æ…‹

        Raises:
            ValueError: reflection resultãŒNoneã®å ´åˆ

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting reflection process...")
        messages = state["messages"]

        refl_prompt = self.settings.subtask_reflection.prompt
        messages.append({"role": "user", "content": refl_prompt.user_prompt})

        try:
            logger.info("Sending request to OpenAI...")
            response = self._chat_parse(
                model=self.settings.subtask_reflection.model_name,
                messages=messages,
                response_format=ReflectionResult,
                **self.settings.subtask_reflection.model_params,
            )
            logger.info("âœ… Successfully received response from OpenAI.")
        except Exception as e:
            logger.error(f"Error during OpenAI request: {e}")
            raise

        reflection_result = response.choices[0].message.parsed
        if reflection_result is None:
            raise ValueError("Reflection result is None")

        messages.append(
            {
                "role": "assistant",
                "content": reflection_result.model_dump_json(),
            }
        )

        update_state = {
            "messages": messages,
            "reflection_results": [reflection_result],
            "challenge_count": state["challenge_count"] + 1,
            "is_completed": reflection_result.is_completed,
        }

        if (
            update_state["challenge_count"] >= self.max_challenge_count
            and not reflection_result.is_completed
        ):
            update_state["subtask_answer"] = (
                f"{state['subtask']}ã®å›ç­”ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
            )

        logger.info("Reflection complete!")
        return update_state

    def _create_answer(self, state: AgentState) -> dict:
        """3. æœ€çµ‚å›ç­”ä½œæˆï½œå…¨ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­”ã‚’çµ±åˆ

        Args:
            state (AgentState): å…¥åŠ›ã®çŠ¶æ…‹

        Returns:
            dict: æ›´æ–°ã•ã‚ŒãŸçŠ¶æ…‹
        """

        logger.info("ğŸš€ Starting final answer creation process...")
        # ã‚µãƒ–ã‚¿ã‚¹ã‚¯çµæœã®ã†ã¡ã‚¿ã‚¹ã‚¯å†…å®¹ã¨å›ç­”ã®ã¿ã‚’å–å¾—
        subtask_results_seq = state.get("subtask_results", [])
        subtask_results = [
            (result.task_name, result.subtask_answer) for result in subtask_results_seq
        ]
        final_answer_prompt = self.settings.final_answer.prompt
        conversation_context = self._format_chat_history(state.get("chat_history", []))
        messages: list[ChatCompletionMessageParam] = [
            {
                "role": "system",
                "content": final_answer_prompt.system_prompt.replace(
                    "{conversation_context}",
                    conversation_context,
                ).replace("{subtask_results}", str(subtask_results)),
            },
            {
                "role": "user",
                "content": final_answer_prompt.user_prompt.replace(
                    "{query}", str(state["query"])
                ),
            },
        ]

        try:
            logger.info("Sending request to OpenAI...")
            response = self._chat_create(
                model=self.settings.final_answer.model_name,
                messages=messages,
                **self.settings.final_answer.model_params,
            )
            logger.info("âœ… Successfully received response from OpenAI.")
        except Exception as e:
            logger.error(f"Error during OpenAI request: {e}")
            raise

        logger.info("Final answer creation complete!")

        return {"answer": response.choices[0].message.content}

    def _execute_subgraph(self, state: AgentState):
        """å˜ä¸€ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã®ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’å®Ÿè¡Œã™ã‚‹ã€‚

        ä¸ãˆã‚‰ã‚ŒãŸ `current_step` ã®ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ã€
        ãƒ„ãƒ¼ãƒ«é¸æŠâ†’ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œâ†’å›ç­”ç”Ÿæˆâ†’å†…çœï¼ˆå¿…è¦ã«å¿œã˜ã¦ãƒ«ãƒ¼ãƒ—ï¼‰
        ã‚’å®Ÿè¡Œã—ã€`Subtask` çµæœã‚’1ä»¶è¿”ã—ã¾ã™ã€‚

        Args:
            state (AgentState): ãƒ¡ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ï¼ˆquery/plan/current_step ãªã©ï¼‰ã€‚

        Returns:
            dict: `subtask_results`ï¼ˆList[Subtask]ï¼‰ã‚’å«ã‚€å·®åˆ†ã€‚
        """
        subgraph = self._create_subgraph()

        result = subgraph.invoke(
            {
                "query": state["query"],
                "plan": state["plan"],
                "subtask": state["plan"][state["current_step"]],
                "current_step": state["current_step"],
                "is_completed": False,
                "challenge_count": 0,
            }
        )

        subtask_result = Subtask(
            task_name=result["subtask"],
            tool_results=result["tool_results"],
            reflection_results=result["reflection_results"],
            is_completed=result["is_completed"],
            subtask_answer=result["subtask_answer"],
            challenge_count=result["challenge_count"],
        )

        return {"subtask_results": [subtask_result]}

    def _should_continue_exec_subtasks(self, state: AgentState) -> list:
        """å…¨ã‚µãƒ–ã‚¿ã‚¹ã‚¯ã«ä¸¦åˆ—é€ä¿¡ã™ã‚‹ãŸã‚ã®åˆ†å²ã‚’ç”Ÿæˆã™ã‚‹ã€‚

        ä¸ãˆã‚‰ã‚ŒãŸè¨ˆç”» `plan` ã®å„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¯¾ã—ã¦ã€
        `execute_subtasks` ã¸é€ã‚‹ `Send` ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

        Args:
            state (AgentState): ãƒ¡ã‚¤ãƒ³ã‚°ãƒ©ãƒ•ã®çŠ¶æ…‹ï¼ˆplan ã‚’å«ã‚€ï¼‰ã€‚

        Returns:
            list: `Send` ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ãƒªã‚¹ãƒˆã€‚
        """
        return [
            Send(
                "execute_subtasks",
                {
                    "query": state["query"],
                    "plan": state["plan"],
                    "current_step": idx,
                },
            )
            for idx, _ in enumerate(state["plan"])
        ]

    def _should_continue_exec_subtask_flow(
        self, state: AgentSubGraphState
    ) -> Literal["end", "continue"]:
        """ã‚µãƒ–ã‚¿ã‚¹ã‚¯å†…ã®ãƒ«ãƒ¼ãƒ—ç¶™ç¶š/çµ‚äº†ã‚’åˆ¤å®šã™ã‚‹ã€‚

        å†…çœçµæœã® `is_completed` ãŒçœŸã€ã¾ãŸã¯æŒ‘æˆ¦å›æ•°ãŒ
        `max_challenge_count` ã«åˆ°é”ã—ãŸå ´åˆã¯çµ‚äº†ã€ãã‚Œä»¥å¤–ã¯ç¶™ç¶šã€‚

        Args:
            state (AgentSubGraphState): ã‚µãƒ–ã‚¿ã‚¹ã‚¯å®Ÿè¡Œä¸­ã®çŠ¶æ…‹ã€‚

        Returns:
            Literal["end", "continue"]: ç¶™ç¶šãƒ•ãƒ©ã‚°ã€‚
        """
        if (
            state["is_completed"]
            or state["challenge_count"] >= self.max_challenge_count
        ):
            return "end"
        else:
            return "continue"

    def _create_subgraph(self) -> Pregel:
        """ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã™ã‚‹

        Returns:
            Pregel: ã‚µãƒ–ã‚°ãƒ©ãƒ•
        """
        workflow = StateGraph(AgentSubGraphState)

        # ãƒ„ãƒ¼ãƒ«é¸æŠãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("select_tools", self._select_tools)

        # ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("execute_tools", self._execute_tools)

        # ã‚µãƒ–ã‚¿ã‚¹ã‚¯å›ç­”ä½œæˆãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("create_subtask_answer", self._create_subtask_answer)

        # ã‚µãƒ–ã‚¿ã‚¹ã‚¯å†…çœãƒãƒ¼ãƒ‰ã‚’è¿½åŠ 
        workflow.add_node("reflect_subtask", self._reflect_subtask)

        # ãƒ„ãƒ¼ãƒ«é¸æŠã‹ã‚‰ã‚¹ã‚¿ãƒ¼ãƒˆ
        workflow.add_edge(START, "select_tools")

        # ãƒãƒ¼ãƒ‰é–“ã®ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        workflow.add_edge("select_tools", "execute_tools")
        workflow.add_edge("execute_tools", "create_subtask_answer")
        workflow.add_edge("create_subtask_answer", "reflect_subtask")

        # ã‚µãƒ–ã‚¿ã‚¹ã‚¯å†…çœãƒãƒ¼ãƒ‰ã®çµæœã‹ã‚‰ç¹°ã‚Šè¿”ã—ã®ãŸã‚ã®ã‚¨ãƒƒã‚¸ã‚’è¿½åŠ 
        workflow.add_conditional_edges(
            "reflect_subtask",
            self._should_continue_exec_subtask_flow,
            {"continue": "select_tools", "end": END},
        )

        app = workflow.compile()

        return app

    def _chat_parse(
        self,
        *,
        model: str,
        messages: Iterable[ChatCompletionMessageParam],
        response_format: Type[BaseModel],
        **rest: Any,
    ):
        """æ§‹é€ åŒ–å‡ºåŠ›ï¼ˆparseï¼‰ã§Chat Completionsã‚’å‘¼ã³å‡ºã™ãƒ˜ãƒ«ãƒ‘ã€‚

        Args:
            model (str): ãƒ¢ãƒ‡ãƒ«åã€‚
            messages (Iterable[ChatCompletionMessageParam]): ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ—ã€‚
            response_format (Type[BaseModel]): Pydanticãƒ¢ãƒ‡ãƒ«å‹ï¼ˆæ§‹é€ åŒ–å‡ºåŠ›ï¼‰ã€‚
            **rest: è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtemperature ç­‰ï¼‰ã€‚

        Returns:
            Any: OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚
        """
        return self.client.beta.chat.completions.parse(
            model=model,
            messages=messages,
            response_format=response_format,
            **rest,
        )

    def _chat_create(
        self,
        *,
        model: str,
        messages: Iterable[ChatCompletionMessageParam],
        **rest: Any,
    ):
        """é€šå¸¸ã®Chat Completionsã‚’å‘¼ã³å‡ºã™ãƒ˜ãƒ«ãƒ‘ã€‚

        Args:
            model (str): ãƒ¢ãƒ‡ãƒ«åã€‚
            messages (Iterable[ChatCompletionMessageParam]): ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸åˆ—ã€‚
            **rest: è¿½åŠ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆtools ç­‰ï¼‰ã€‚

        Returns:
            Any: OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã€‚
        """
        return self.client.chat.completions.create(
            model=model,
            messages=messages,
            **rest,
        )

    def _format_chat_history(
        self, chat_history: list[ChatCompletionMessageParam]
    ) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼/ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®å±¥æ­´ã®ã¿ã‚’æ–‡å­—åˆ—ã«æ•´å½¢ã™ã‚‹ã€‚

        - roleãŒuser/assistantä»¥å¤–ï¼ˆsystem/toolãªã©ï¼‰ã¯é™¤å¤–
        - è¡¨ç¤ºãƒ©ãƒ™ãƒ«ã¯æ—¥æœ¬èªåŒ–ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼/ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆï¼‰
        - chat_history_max_turnsãŒæŒ‡å®šã•ã‚Œã¦ã„ã‚Œã°æœ«å°¾ã‹ã‚‰ãã®ä»¶æ•°ã‚’æ¡ç”¨
        """
        if not chat_history:
            return ""

        filtered = [m for m in chat_history if m.get("role") in ("user", "assistant")]

        # æœ«å°¾Nä»¶ã«åˆ¶é™ï¼ˆNoneãªã‚‰å…¨ä»¶ï¼‰
        if self.chat_history_max_turns is not None and self.chat_history_max_turns > 0:
            filtered = filtered[-self.chat_history_max_turns :]  # NOQA: E203

        role_map = {"user": "ãƒ¦ãƒ¼ã‚¶ãƒ¼", "assistant": "ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ"}
        lines: list[str] = []
        for m in filtered:
            role = role_map.get(m.get("role", ""), "")
            content = str(m.get("content", "")).strip()
            if role and content:
                lines.append(f"{role}: {content}")
        return "\n".join(lines)
